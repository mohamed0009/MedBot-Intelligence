"""
Configuration management for LLM QA Module
"""

from pydantic_settings import BaseSettings
from typing import List, Optional
import os


class Settings(BaseSettings):
    """Application settings"""
    
    # Service
    SERVICE_NAME: str = "llm-qa-module"
    LOG_LEVEL: str = "INFO"
    
    # Database
    DATABASE_URL: str = "postgresql://docqa_admin:changeme@postgres:5432/llm_qa"
    
    # RabbitMQ
    RABBITMQ_HOST: str = "rabbitmq"
    RABBITMQ_PORT: int = 5672
    RABBITMQ_USER: str = "docqa_rabbitmq"
    RABBITMQ_PASS: str = "changeme"
    RABBITMQ_CONSUME_QUEUE: str = "indexed_documents"
    
    # LLM Configuration
    LLM_PROVIDER: str = "openai"  # openai, huggingface, local
    
    # OpenAI Configuration
    OPENAI_API_KEY: Optional[str] = None
    OPENAI_MODEL: str = "gpt-4-turbo-preview"  # gpt-4, gpt-3.5-turbo
    OPENAI_TEMPERATURE: float = 0.7
    OPENAI_MAX_TOKENS: int = 2000
    
    # HuggingFace / Local LLM Configuration
    LOCAL_MODEL_NAME: str = "meta-llama/Llama-2-7b-chat-hf"
    LOCAL_MODEL_DEVICE: str = "cpu"  # cpu or cuda
    HF_TOKEN: Optional[str] = None
    
    # RAG Configuration
    RETRIEVAL_TOP_K: int = 5  # Number of chunks to retrieve
    RETRIEVAL_MIN_SIMILARITY: float = 0.7
    ENABLE_RERANKING: bool = True
    RERANKING_TOP_K: int = 3
    
    # Prompt Configuration
    SYSTEM_PROMPT: str = """You are a medical AI assistant helping healthcare professionals analyze clinical documents.
Your role is to provide accurate, evidence-based answers based ONLY on the provided context.

Rules:
1. Only answer based on the provided context
2. If information is not in the context, say "I don't have enough information"
3. Always cite your sources with [Source X] notation
4. Be concise but comprehensive
5. Use medical terminology appropriately
6. Never make up information or hallucinate
"""
    
    QUESTION_PROMPT_TEMPLATE: str = """Context from clinical documents:
{context}

Question: {question}

Please provide a detailed answer based on the context above. Include citations [Source X] for each fact."""
    
    # API Configuration
    MAX_QUERY_LENGTH: int = 500
    ENABLE_STREAMING: bool = True
    ENABLE_CITATIONS: bool = True
    
    # Search Service Integration
    SEARCH_SERVICE_URL: str = "http://indexeur-semantique:8000"
    
    # CORS
    CORS_ORIGINS: List[str] = ["http://localhost:3000", "http://localhost:8000"]
    
    # Performance
    WORKERS: int = 4
    CACHE_TTL_SECONDS: int = 3600
    
    class Config:
        env_file = ".env"
        case_sensitive = True


settings = Settings()
